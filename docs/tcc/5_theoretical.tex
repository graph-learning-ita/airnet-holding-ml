\chapter[Theoretical Framework]{Theoretical Framework and Related Works}
\label{TheoreticalFramework}

Graph machine learning can be tracked backwards to the problem of `learning' on data that is inherently a graph \cite{silva2016machine, JMLR:Perozzi} or can be modeled as a graph \cite{verri2013,grape2020}. This field encompasses a variety of tasks, including node/edge classification, network construction, link prediction, graph classification, graph cut/partitioning, network embeddings, graph coarsening/reduction, which rely on learning representations from graph-structured data. Over the last decades, researchers have developed numerous approaches to tackle these challenges, initially these techniques were most developed by complex networks researchers. However, in the last decade with the advancements in deep learning, the field has seen a significant shift towards the merging of three main communities: graph signal processing, deep learning and complex nets.

As described, defining the field of graph machine learning is not straightforward, as it encompasses a broad range of methods and applications. The tasks mentioned above are just a few examples of the many challenges that can be addressed through graph-based learning techniques. For clarity, these tasks can be categorized into three main learning paradigms: supervised, unsupervised, and semi-supervised learning. In this study, we are interested on the (semi-)supervised learning paradigm, which encompasses a variety of techniques designed to leverage learning to (partially-)labeled data \cite{verri2018advantages,amanciof}. But we can refine even more, in fact, this work will focus in the subset of graph elements prediction(classification/regression) methods.

In this chapter, we provide an overview of the theoretical framework of graph machine learning for node/edge prediction. Here we consider the division of the field into \texttt{classical} graph learning and \texttt{deep} graph learning, where here `classical' refers to the machine learning techniques applied to graphs before the advent of graph neural networks, where standard ML algorithms were applied to graph data and the topological information measures were encoded as features together with the tabular data  \cite{costa2007characterization, silva2016machine}. This bipartition is what will pave the way of our explanation, since the last decade has seen a complex interplay between these two approaches. The field's evolution can be traced back to when \citeonline{bruna2013spectral} introduced one of the first GNN architectures leaned on the theory of graph signal processing. Concurrently, researchers were developing node embedding techniques like DeepWalk \cite{perozzi2014deepwalk} and node2vec \cite{grover2016node2vec}, which bridged classical and deep approaches while remaining using complex networks concepts. The subsequent years saw a surge in GNN architectures, including Graph Convolutional Networks \cite{kipf2016semi} and GraphSAGE \cite{hamilton2017inductive}, marking a shift towards more sophisticated deep learning approaches for graphs and the unification of the field.  

In the following sections, we explain each subset, their theory and applications, and how they have evolved over time. We also discuss the challenges and limitations of these methods.

\section{Classical graph learning}



These early efforts focused on shallow learning techniques such as feature engineering, graph traversal algorithms, and spectral methods, which laid the foundation for understanding graph structure and dynamics. Methods like community detection, centrality measures, and link prediction became key tools for analyzing large-scale networks in areas such as social science, biology, and infrastructure systems. By modeling relationships as graphs, these approaches enabled researchers to capture both local and global properties, leading to significant insights in network theory and real-world applications.

\section{Deep graph learning}

The rise of deep learning has revolutionized the field of graph machine learning, enabling the development of more powerful and scalable models for graph data. Graph neural networks can be divide in two main categories: spectral-based and spatial-based. Here is a trick thing, the GCN architecture \cite{kipf2016semi} is commonly divulgated as a spatial-based method, since it is more intuitive talking about the convolution operation in the spatial domain, where we simply aggregate information from the immediate neighbors. However, the GCN is a spectral-based method, in fact, it can be thought as a simplification of the first spectral GNN \cite{bruna2013spectral} proposed and that builds the math behind GCNs. That said, first we introduce the spectral-based GNNs and then the spatial-based ones.

\subsection{Spectral-based GNNs}

Spectral methods are rooted in graph signal processing. The core idea is that a signal on a graph can be represented as node features, where each feature vector at a node corresponds to a `signal' defined over the graph. In this context, the graph Laplacian $\mathcal{L} = D - A$, where $D$ is the degree matrix and $A$ is the adjacency matrix, plays a crucial role. It captures the structure of the graph and can be used to perform operations analogous to Fourier transforms in classical signal processing. Spectral methods can be categorized into two types: eigenvalue-based, where the focus is on creating a graph filter in the Fourier domain, and eigenvector-based, where the goal is to use a spectral basis to decompose the signal \cite{bo2023surveyspectralgraphneural}.

\citeonline{bruna2013spectral} introduced the first spectral Graph Neural Network (GNN), termed the Spectral CNN (SCNN), which aimed to translate ideas from standard Convolutional Neural Networks for images to graphs. The SCNN leverages the spectral decomposition of the graph Laplacian $\mathcal{L} = U \Lambda U^T$ to define a filter convolution operation in the Fourier domain. In this framework, the graph Fourier transform of a signal $f$ is represented as $\hat{f} = U^T f$, and the convolution operation ($\star$) is defined as $g_{\theta} \star f = U g_{\theta} U^T f$, where $g_{\theta}$ is a learnable filter parameterized by $\theta$. While powerful, the SCNN faces significant challenges: it requires $\mathcal{O}(n^3)$ computational complexity to calculate the entire graph spectrum, which is prohibitively expensive for large graphs. Moreover, the non-localized nature of eigenvectors means global information can overshadow local structural details, leading suboptimal balance between local and global information aligned with a huge parameter complexity \cite{usgnn}.

To address these limitations, \citeonline{defferrard2016convolutional} introduces Chebyshev polynomials to approximate spectral filters, effectively reducing computational complexity while preserving the ability to capture localized patterns in the graph structure. The main ideia is to redefine our previous filtering operation to $ g_{\theta}(\mathcal{L} ) f = \sum_{k=0}^{K-1} \theta_k T_k(\widetilde{\mathcal{L}}) f $, where $T_k(\widetilde{\mathcal{L}}) = $ is the Chebyshev polinomial of order k evaluated at the scaled Laplacian $\widetilde{\mathcal{L}} = 2 \frac{\mathcal{L}}{\lambda_\text{max}} - I_n$. This innovation not only makes spectral GNNs more scalable to larger graphs, since we just need to calculate the first eigenpair ($\mathcal{O}(n^2)$ through the power method) for the approximations, but also enhances their ability to balance local and global information processing. In fact, the filters are $K$-localized for polinomials of order $K$, that is intuitive by remembering that $\mathcal{L} ^K$ represents the paths with length less or equal to $K$.  This work build the way to