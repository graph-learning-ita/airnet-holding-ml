\chapter[Theoretical Framework]{Theoretical Framework and Related Works}
\label{TheoreticalFramework}

Graph machine learning can be tracked backwards to the problem of `learning' on data that is inherently a graph \cite{silva2016machine, JMLR:Perozzi} or can be modeled as a graph \cite{verri2013,grape2020}. This field encompasses a variety of tasks, including node/edge classification, network construction, link prediction, graph classification, graph cut/partitioning, network embeddings, graph coarsening/reduction, which rely on learning representations from graph-structured data. Over the last decades, researchers have developed numerous approaches to tackle these challenges, initially these techniques were most developed by complex networks researchers. However, in the last decade with the advancements in deep learning, the field has seen a significant shift towards the merging of three main communities: graph signal processing, deep learning and complex nets.

As described, defining the field of graph machine learning is not straightforward, as it encompasses a broad range of methods and applications. The tasks mentioned above are just a few examples of the many challenges that can be addressed through graph-based learning techniques. For clarity, these tasks can be categorized into three main learning paradigms: supervised, unsupervised, and semi-supervised learning. In this study, we are interested on the (semi-)supervised learning paradigm, which encompasses a variety of techniques designed to leverage learning to (partially-)labeled data \cite{verri2018advantages,amanciof}. But we can refine even more, in fact, this work will focus in the subset of graph elements prediction(classification/regression) methods.

In this chapter, we provide an overview of the theoretical framework of graph machine learning for node/edge prediction. Here we consider the division of the field into \texttt{traditional} graph learning and \texttt{deep} graph learning, where here `traditional' refers to the machine learning techniques applied to graphs before the advent of graph neural networks, where standard ML algorithms were applied to graph data and the topological information measures were encoded as features together with the tabular data  \cite{costa2007characterization, silva2016machine}. This bipartition is what will pave the way of our explanation, since the last decade has seen a complex interplay between these two approaches. The field's evolution can be traced back to when \citeonline{bruna2013spectral} introduced one of the first GNN architectures leaned on the theory of graph signal processing. Concurrently, researchers were developing node embedding techniques like DeepWalk \cite{perozzi2014deepwalk} and node2vec \cite{grover2016node2vec}, which bridged traditional and deep approaches while remaining using complex networks concepts. The subsequent years saw a surge in GNN architectures, including Graph Convolutional Networks(GCNs) \cite{kipf2016semi} and GraphSAGE \cite{hamilton2017inductive}, marking a shift towards more sophisticated deep learning approaches for graphs and the unification of the field.  

In the following sections, we explain each subset, their theory and applications, and how they have evolved over time. We also discuss the challenges and limitations of these methods.

\section{traditional graph learning}

\label{classical_learning}

These early efforts focused on shallow learning techniques such as feature engineering, graph traversal algorithms, and spectral methods, which laid the foundation for understanding graph structure and dynamics. Methods like community detection, centrality measures, and link prediction \cite{silva2016machine} became key tools for analyzing large-scale networks in areas such as social science, biology, and infrastructure systems \cite{newman2018networks,boccaletti2006complex}. A significant focus of these techniques was to develop graph-based features that could be integrated into traditional machine learning models, effectively transforming graph data into a format compatible with standard algorithms like logistic regression, decision trees, and support vector machines. By encoding graph topology through hand-crafted features, such as connectivity and centrality, researchers could leverage these features for tasks like classification, regression, and clustering in tabular machine learning frameworks.

Among these features, centrality measures became particularly important due to their ability to capture the relative importance or influence of nodes in a graph, not just nodes \cite{bonacich1987power}, but other graph elements such as edges \cite{Lu2013edgebetw, brohl2019centrality} and hyperedges \cite{tudisco2021hyperedge}. Centrality measures, such as degree, betweenness, and closeness, served as input features in machine learning pipelines, helping to predict outcomes based on the structural role of nodes within the network. 

Spectral centrality, particularly eigenvector centrality \cite{bonacich1987power}, has proven valuable in machine learning applications due to its ability to identify globally influential nodes. Eigenvector centrality assigns a score to each node by considering not only its direct connections but also the centrality of its neighbors, which results in a recursive definition. Mathematically, the eigenvector centrality $x$ of a node in a graph can be defined as the solution to the equation $Ax = \lambda x$, where $A$ is the adjacency matrix of the graph, and $\lambda$ is the largest eigenvalue, thus $x$ is the eigenvector associated with the largest eigenvalue. This relationship arises from the fact that the centrality of a node is proportional to the sum of the centralities of its neighbors, if we normalize the adjacency we get an stochastic matrix and then $\lambda =1 $ is the largest eigenvalue, named the \texttt{Perron vector}. The eigenvector centrality captures both local and global structure in a network, making it a powerful feature for tasks such as node classification, ranking, and recommendation systems. A related and widely used spectral measure is PageRank \cite{brin1998pagerank}, which extends the idea of eigenvector centrality by introducing a damping factor to model random surfing behavior,
\[
PR(v) = \frac{1 - d}{N} + d \sum_{u \in \mathcal{N}(v)} \frac{PR(u)}{\text{deg}(u)},
\]
where $PR(v)$ is the PageRank score of node $v$, $d$ is the damping factor, and $\mathcal{N}(v)$ represents the neighbors of node $v$. This iterative computation converges to a stationary distribution of scores, which can be interpreted as the probability of landing on a given node after a long random walk, in this sense the \texttt{Perron vector} signifies the convergence of the process in the infinite. PageRank has been widely used in ranking tasks, such as identifying important websites in search engines or recommending influential users in social networks.

However, these spectral-based centralities come with limitations. Eigenvector centrality requires the computation of the principal eigenvector of the adjacency matrix, which involves finding the largest eigenpair problem. This has a time complexity of $\mathcal{O}(n^2 d)$ for exact methods, where $n$ is the number of nodes in the graph and $d$ is the ratio of convergence for the power method. Furthermore, spectral methods can suffer from limitations rooted in the Perron-Frobenius theorem, which guarantees the existence of a unique largest eigenvalue only for irreducible, non-negative matrices. For graphs that are disconnected or have negative weights, these conditions are violated, and the eigenvector centrality may not be well-defined or interpretable. That is, the adjacency matrix should be non-negative and irreducible, where we could use the Perron test $\sum A^k > 0$ to see if the graph is strongly connected.  These centralities also tend to be node-centric, lacking a direct extension to edge importance. For edge centrality, betweenness remains crucial, particularly in directed graphs, where the structural role of links (edges) must be considered to capture flow dynamics. Additionally, spectral centralities can be sensitive to noise and small perturbations in the graph structure, leading to instability in the centrality scores. Despite these challenges, spectral centrality remains a powerful tool for machine learning tasks that benefit from capturing global graph structure, provided that the computational and stability issues can be managed.


\section{Deep graph learning}

\label{deep_learning}

The rise of deep learning has revolutionized the field of graph machine learning, enabling the development of more powerful and scalable models for graph data. Graph neural networks can be divide in two main categories: spectral-based and spatial-based. Here is a trick thing, the GCN architecture \cite{kipf2016semi} is commonly divulgated as a spatial-based method, since it is more intuitive talking about the convolution operation in the spatial domain, where we simply aggregate information from the immediate neighbors. However, the GCN is a spectral-based method, in fact, it can be thought as a simplification of the first spectral GNN \cite{bruna2013spectral} proposed and that builds the math behind GCNs. That said, first we introduce the spectral-based GNNs and then the spatial-based ones.

\subsection{Spectral-based GNNs}

Spectral methods are rooted in graph signal processing. The core idea is that a signal on a graph can be represented as node features, where each feature vector at a node corresponds to a `signal' defined over the graph. In this context, the graph Laplacian $\mathcal{L} = D - A$, where $D$ is the degree matrix and $A$ is the adjacency matrix, plays a crucial role. It captures the structure of the graph and can be used to perform operations analogous to Fourier transforms in traditional signal processing. Spectral methods can be categorized into two types: eigenvalue-based, where the focus is on creating a graph filter in the Fourier domain, and eigenvector-based, where the goal is to use a spectral basis to decompose the signal \cite{bo2023surveyspectralgraphneural}.

\citeonline{bruna2013spectral} introduced the first spectral Graph Neural Network (GNN), termed the Spectral CNN (SCNN), which aimed to translate ideas from standard Convolutional Neural Networks for images to graphs. The SCNN leverages the spectral decomposition of the graph Laplacian $\mathcal{L} = U \Lambda U^T$ to define a filter convolution operation in the Fourier domain. In this framework, the graph Fourier transform of a signal $f$ is represented as $\hat{f} = U^T f$, and the convolution operation ($\star$) is defined as $g_{\theta} \star f = U g_{\theta} U^T f$, where $g_{\theta}$ is a learnable filter parameterized by $\theta$. While powerful, the SCNN faces significant challenges: it requires $\mathcal{O}(n^3)$ computational complexity to calculate the entire graph spectrum, which is prohibitively expensive for large graphs. Moreover, the non-localized nature of eigenvectors means global information can overshadow local structural details, leading suboptimal balance between local and global information aligned with a huge parameter complexity \cite{usgnn}.

To address these limitations, ChebNet\citeonline{defferrard2016convolutional} introduces Chebyshev polynomials to approximate spectral filters, effectively reducing computational complexity while preserving the ability to capture localized patterns in the graph structure. The main ideia is to redefine our previous filtering operation to $ g_{\theta}(\mathcal{L} ) f = \sum_{k=0}^{K-1} \theta_k T_k(\widetilde{\mathcal{L}}) f $, where $T_k(\widetilde{\mathcal{L}}) = $ is the Chebyshev polinomial of order k evaluated at the scaled Laplacian $\widetilde{\mathcal{L}} = 2 \frac{\mathcal{L}}{\lambda_\text{max}} - I_n$. This innovation not only makes spectral GNNs more scalable to larger graphs, since we just need to calculate the first eigenpair ($\mathcal{O}(n^2)$ through the power method) for the approximations, but also enhances their ability to balance local and global information processing. In fact, the filters are $K$-localized for polinomials of order $K$, that is intuitive by remembering that $\mathcal{L} ^K$ represents the paths with length less or equal to $K$.  
The ChebNet laid the foundation for GCNs \cite{kipf2016semi}. Although GCNs are commonly referred to as spatial methods, their underlying principle is rooted in the truncation of the Chebyshev expansion to $K=1$, which limits the filter to first-order neighbors. This simplification reduces computational complexity significantly while preserving effectiveness. Instead of requiring the full spectral decomposition of the Laplacian matrix, GCNs use a localized approximation of the graph convolution, expressed as: $g_{\theta} \star f \approx \theta (I_n + \widetilde{A}) f$, where $\widetilde{A} = D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ is the normalized adjacency matrix, where $A$ is the adjacency matrix, and $D$ is the degree matrix. This approximation results in an efficient propagation rule that aggregates information from a node's immediate neighbors while updating the node's features. This propagation mechanism is often confused as a spatial method because it effectively propagates information from adjacent nodesâ€”akin to a spatial neighborhood aggregation. Although its already a simple model, results have shown that GCNs can achieve state-of-the-art performance on a variety of tasks with even more simplifications \cite{wu2019simplifying}.
However, as we can note, all these spectral methods works just in undirected graphs, since it needs the spectral decomposition. Furthermore, these methods are `node centric', that is, they focus just on node features and the topology of the nodes, most of this is because the adjacency matrix maps the dimension of nodes to nodes, thus leaving \emph{edge features} out of the scene.

\subsection{Spatial-based GNNs}

\label{spatial-based}

Spatial-based GNNs differ from spectral-based approaches by directly leveraging the graph structure to perform convolutions in the spatial domain, rather than relying on the spectral decomposition of graph operators like the Laplacian. In spatial-based methods, the convolution operation is interpreted as an aggregation of node features from a node's local neighborhood, akin to how standard convolutional neural networks aggregate pixel information from nearby regions in image data. These methods operate by iteratively updating node representations by propagating information between neighboring nodes, making them intuitive and highly scalable for large-scale graphs.

The general framework for message passing in spatial-based GNNs can be described as follows. For each node $i$, at layer $t$, we aggregate the features of its neighbors $\mathcal{N}(i)$ to produce an updated node embedding: $\mathbf{m}_i^{(t+1)} = \text{AGGREGATE}^{(t)} \left( \left\{ \mathbf{h}_j^{(t)} : j \in \mathcal{N}(i) \right\} \right)$, where $\mathbf{h}_j^{(t)}$ is the feature of neighboring node $j$ at layer $t$. Then, we update the node $i$'s representation: $\mathbf{h}_i^{(t+1)} = \text{UPDATE}^{(t)} \left( \mathbf{h}_i^{(t)}, \mathbf{m}_i^{(t+1)} \right)$, where $\text{AGGREGATE}^{(t)}$ is a neighborhood aggregation function, and $\text{UPDATE}^{(t)}$ is the node update function.

The general idea behind spatial-based GNNs is that, for each node, we aggregate the features of its neighbors to produce an updated node embedding. A key example of this is the GraphSAGE architecture \cite{hamilton2017inductive}, which computes node representations by sampling and aggregating features from the node's neighbors. The GraphSAGE model employs several types of aggregation functions, including mean, LSTM-based, and pooling aggregators, which allow for flexible and inductive learning on large graphs. In particular, GraphSAGE enables the generation of embeddings for unseen nodes, making it suitable for inductive learning tasks, where the model needs to generalize to new nodes that were not present during training. Unlike spectral-based methods, which are constrained to a fixed graph size and structure due to their reliance on the graph Laplacian, spatial-based GNNs are inherently more flexible and can be applied to dynamic and evolving graphs. These models perform neighborhood aggregation locally, and therefore do not require the global knowledge of the graph structure that spectral methods need. This flexibility makes them particularly useful for large-scale graphs and for graphs where the structure may change over time, such as social networks or knowledge graphs.


Another prominent spatial-based GNN is the Graph Attention Network (GAT) \cite{velickovic2017graph}, which introduced attention mechanisms into graph learning. GAT models learn to assign different weights to the neighbors of a node, allowing the model to focus more on the most relevant neighbors during the feature aggregation process. This is achieved using a self-attention mechanism, where the importance of neighboring nodes is learned through a shared attention coefficient, $
e_{ij} = \text{LeakyReLU}(\mathbf{a}^T [\mathbf{W} \mathbf{h}_i || \mathbf{W} \mathbf{h}_j]) $, where $e_{ij}$ represents the attention coefficient between nodes $i$ and $j$, $\mathbf{W}$ is a learnable weight matrix, $\mathbf{h}_i$ and $\mathbf{h}_j$ are the feature vectors of nodes $i$ and $j$, and $||$ denotes concatenation. The attention coefficients are then normalized across all of a node's neighbors using the softmax function, $
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})}
$, this normalization ensures that the attention coefficients sum to 1, allowing the model to perform a weighted aggregation of the neighbors' features, $
\mathbf{h}_i' = \sigma \left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} \mathbf{W} \mathbf{h}_j \right)$,
here $\mathbf{h}_i'$ is the updated representation of node $i$, and $\sigma$ is a non-linear activation function. By learning attention coefficients, GATs can capture both the importance and the structure of the graph, making them particularly effective in tasks where the relationships between nodes are not equally important, such as in citation networks or social media graphs.
